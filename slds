ğŸ“˜ Naive Method â€“ Dictionary Index

"Cat" â†’ 23, "Car" â†’ 24

âŒ Close IDs, but no relation in meaning

ğŸ“Š Frequency-Based Encoding â€“ Co-occurrence Across Documents

We build a matrix:

Rows = words (e.g., "oil", "economy", "banana")

Columns = documents

Cells = how often the word appears in each doc

Use cosine similarity between word vectors

"Oil" and "Economy" â†’ often appear in same documents â†’ high similarity

â— But: concepts are still not semantically similar

ğŸ§  Word Embeddings â€“ Word2Vec / GloVe

Learn meaning from surrounding words, not just document co-occurrence

"Loan" is close to "Credit", far from "Banana"

âœ… Captures semantic relationships

âœ… Allows analogies: King - Man + Woman â‰ˆ Queen
