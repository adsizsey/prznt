🔁 Why RNNs? Why Not Regular Neural Networks?
Subhead: “When word order matters, memory helps.”

This slide sets up the motivation for Recurrent Neural Networks by contrasting them with fully connected (feedforward) networks.

✅ Key Points to Include:
🧱 Standard neural networks treat all inputs as fixed-size — no sense of sequence.

🧠 But in language, order matters:

“The cat chased the dog” ≠ “The dog chased the cat”

🔁 RNNs introduce memory: each word's representation depends on previous words.

📉 Problem: classic RNNs struggle with long-term dependencies due to vanishing gradients.
