🔁 Why RNNs? Why Not Regular Neural Networks?
Subhead: “When word order matters, memory helps.”

🧱 The Problem:

Feedforward neural networks treat input as unordered — not ideal for language, where order = meaning.

“The cat chased the dog” ≠ “The dog chased the cat”

🔁 Enter RNNs:

RNNs pass information forward through time steps, building memory.

Each word’s meaning is shaped by what came before it.

⚠️ But RNNs Struggle With Long Sentences:

Gradients vanish over time → early words get forgotten.

Long-range dependencies become hard to learn.

✨ That’s where Attention comes in:

Instead of remembering everything step by step...

🔦 Attention lets the model directly focus on relevant words, no matter how far back they are.
