ğŸ” Why RNNs? Why Not Regular Neural Networks?
Subhead: â€œWhen word order matters, memory helps.â€

This slide sets up the motivation for Recurrent Neural Networks by contrasting them with fully connected (feedforward) networks.

âœ… Key Points to Include:
ğŸ§± Standard neural networks treat all inputs as fixed-size â€” no sense of sequence.

ğŸ§  But in language, order matters:

â€œThe cat chased the dogâ€ â‰  â€œThe dog chased the catâ€

ğŸ” RNNs introduce memory: each word's representation depends on previous words.

ğŸ“‰ Problem: classic RNNs struggle with long-term dependencies due to vanishing gradients.
