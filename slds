ğŸ” Why RNNs? Why Not Regular Neural Networks?
Subhead: â€œWhen word order matters, memory helps.â€

ğŸ§± The Problem:

Feedforward neural networks treat input as unordered â€” not ideal for language, where order = meaning.

â€œThe cat chased the dogâ€ â‰  â€œThe dog chased the catâ€

ğŸ” Enter RNNs:

RNNs pass information forward through time steps, building memory.

Each wordâ€™s meaning is shaped by what came before it.

âš ï¸ But RNNs Struggle With Long Sentences:

Gradients vanish over time â†’ early words get forgotten.

Long-range dependencies become hard to learn.

âœ¨ Thatâ€™s where Attention comes in:

Instead of remembering everything step by step...

ğŸ”¦ Attention lets the model directly focus on relevant words, no matter how far back they are.
